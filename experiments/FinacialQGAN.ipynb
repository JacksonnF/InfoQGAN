{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pennylane as qml\n",
    "from  pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import chisquare\n",
    "import plotly\n",
    "\n",
    "import numpy as npreal\n",
    "import wandb\n",
    "\n",
    "import ndtest\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hid_size) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_size (int): The size of the input data.\n",
    "        - hid_size (int): The size of the hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the Discriminator.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the Discriminator.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "    def fit_discriminator(self, x_data, criterion, net_G, optimizer, input_size):\n",
    "        \"\"\"\n",
    "        Trains the Discriminator module.\n",
    "\n",
    "        Args:\n",
    "        - x_data (torch.Tensor): The real data.\n",
    "        - criterion: The loss criterion.\n",
    "        - net_G: The Generator network.\n",
    "        - optimizer: The optimizer.\n",
    "        - input_size (int): The size of the input data.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, _ = x_data.shape[0], x_data.shape[1]\n",
    "        x_rand = torch.rand((batch_size, input_size))\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "        # Forward pass Discriminator on \"real\" data\n",
    "        labels_real = torch.ones((batch_size, 1)) * 0.9\n",
    "        outputs = self.forward(x_data)\n",
    "        loss_d_real = criterion(outputs, labels_real)\n",
    "\n",
    "        # Forward pass Discriminator with \"fake\" data from Generator\n",
    "        g = net_G(x_rand).detach() # Stop gradients from being updated in generator\n",
    "        labels_fk = torch.zeros((batch_size, 1)) + 0.1\n",
    "        outputs = self.forward(g)\n",
    "        loss_d_fake = criterion(outputs, labels_fk)\n",
    "\n",
    "        loss_d = loss_d_fake + loss_d_real\n",
    "        loss_d.backward() # Compute Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        return loss_d.item()\n",
    "\n",
    "number_of_qubits = 4\n",
    "number_of_reps = 5\n",
    "\n",
    "dev = qml.device('default.qubit', wires=number_of_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_generator(inputs, params):\n",
    "    \"\"\"\n",
    "    Defines the quantum generator circuit. Embedding and Ansatz layers were based on the paper we followed.\n",
    "\n",
    "    Args:\n",
    "    - inputs (torch.Tensor): The input data.\n",
    "    - params (torch.Tensor): The parameters of the circuit.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The probabilities of the qubits.\n",
    "    \"\"\"\n",
    "    qml.AngleEmbedding(np.pi*(inputs-0.5)/2, rotation=\"Y\", wires=range(number_of_qubits))\n",
    "    num_params = 0\n",
    "    for layer in range(number_of_reps):\n",
    "        #initial Ry gates.\n",
    "        for i in range(number_of_qubits):\n",
    "            qml.RY(params[num_params], wires=i)\n",
    "            num_params += 1\n",
    "\n",
    "        # Entangling block\n",
    "        for i in range(number_of_qubits):\n",
    "            qml.CZ(wires=[i, (i+1)%number_of_qubits])\n",
    "    return qml.probs(wires=range(number_of_qubits))\n",
    "\n",
    "weights = {\"params\": number_of_qubits * number_of_reps}\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.q_gen = qml.qnn.TorchLayer(quantum_generator, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the Generator.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the Generator.\n",
    "        \"\"\"\n",
    "        return self.q_gen(x)\n",
    "\n",
    "    def fit_generator(self, net_D, batch_size, input_size, criterion, optimizer, code_dim, beta):  \n",
    "        \"\"\"\n",
    "        Trains the Generator module.\n",
    "\n",
    "        Args:\n",
    "        - net_D: The Discriminator network.\n",
    "        - T: The T network.\n",
    "        - batch_size (int): The size of the batch.\n",
    "        - input_size (int): The size of the input data.\n",
    "        - criterion: The loss criterion.\n",
    "        - optimizer: The optimizer.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - beta (float): The beta value.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "        \"\"\"\n",
    "        x_rand = torch.rand((batch_size, input_size))\n",
    "        self.zero_grad()\n",
    "        \n",
    "        # Generate outputs With Generator and check if they fool Discriminator\n",
    "        labels_real = torch.ones((batch_size, 1)) * 0.9\n",
    "        g = self.forward(x_rand)\n",
    "        outputs = net_D(g)\n",
    "\n",
    "        loss_g = criterion(outputs, labels_real) # We want \"fake\" Generator output to look real  \n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss_g.item()\n",
    "\n",
    "class T(nn.Module):\n",
    "    def __init__(self, noise_dim, code_dim, hid_size) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the T module.\n",
    "\n",
    "        Args:\n",
    "        - noise_dim (int): The dimension of the noise.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - hid_size (int): The size of the hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(code_dim + noise_dim, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, gen, codes):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the T module.\n",
    "\n",
    "        Args:\n",
    "        - gen (torch.Tensor): The Generator output.\n",
    "        - codes (torch.Tensor): The code data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the T module.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.layers(torch.cat((gen, codes), dim=1))\n",
    "\n",
    "    def fit_t(self, net_G, batch_size, input_size, code_dim, optimizer):\n",
    "        \"\"\"\n",
    "        Trains the T module.\n",
    "\n",
    "        Args:\n",
    "        - net_G: The Generator network.\n",
    "        - batch_size (int): The size of the batch.\n",
    "        - input_size (int): The size of the input data.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - optimizer: The optimizer.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "        \"\"\"\n",
    "        x_rand = torch.rand((batch_size, input_size))\n",
    "        self.zero_grad()\n",
    "\n",
    "        # Compute loss following similar logic to MINE term in generator loss\n",
    "        with torch.no_grad():\n",
    "            g = net_G(x_rand)\n",
    "        \n",
    "        T_out = self.forward(g, x_rand[:, 0:code_dim])\n",
    "        x_marg = x_rand[torch.randperm(x_rand.size(0)), 0:code_dim]\n",
    "\n",
    "        t_loss = -(torch.mean(T_out) - torch.log(torch.mean(torch.exp(self.forward(g, x_marg)))))\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return t_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the generator\n",
    "inputs = torch.rand(4)\n",
    "params = torch.rand(20)\n",
    "gen = quantum_generator(inputs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_distribution(distribution):\n",
    "    \"\"\"\n",
    "    Graphs the distribution.\n",
    "\n",
    "    Args:\n",
    "    - distribution (torch.Tensor): The probability distribution tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate bin centers\n",
    "    bin_edges = np.linspace(-0.1, 0.1, 17)  # 16 bins means 17 edges\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2  # Average of edges to find center\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.bar(bin_centers, distribution, width=bin_edges[1] - bin_edges[0], align='center', color='blue', alpha=0.7)\n",
    "    plt.xlabel('Return Interval')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title('Probability Distribution of Returns')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch stock data and calculate daily returns\n",
    "def fetch_and_process_data(ticker, start_date, end_date):\n",
    "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    # Calculate daily returns\n",
    "    daily_returns = stock_data['Adj Close'].pct_change().dropna()\n",
    "    # Normalize returns to the [-0.1, 0.1] interval\n",
    "    normalized_returns = np.clip(daily_returns, -0.1, 0.1)\n",
    "    return normalized_returns\n",
    "\n",
    "# Fetch and process AAPL and TSLA data\n",
    "aapl_returns = fetch_and_process_data('AAPL', '2011-01-01', '2022-12-31')\n",
    "tsla_returns = fetch_and_process_data('TSLA', '2011-01-01', '2022-12-31')\n",
    "\n",
    "# Ensure both return series are aligned by date\n",
    "aligned_returns = pd.concat([aapl_returns, tsla_returns], axis=1, join='inner').dropna()\n",
    "\n",
    "# Separate the aligned data back into AAPL and TSLA returns\n",
    "aapl_returns, tsla_returns = aligned_returns.iloc[:, 0], aligned_returns.iloc[:, 1]\n",
    "\n",
    "# Generate Portfolio Returns\n",
    "num_datasets = 2000\n",
    "num_bins = 16\n",
    "datasets = []\n",
    "\n",
    "for _ in range(num_datasets):\n",
    "    alpha = np.random.uniform(0, 1)  # Randomly select alpha\n",
    "    portfolio_returns = alpha * aapl_returns + (1 - alpha) * tsla_returns\n",
    "    \n",
    "    # Create Probability Distribution Datasets\n",
    "    bin_edges = np.linspace(-0.1, 0.1, num_bins+1)\n",
    "    hist, _ = np.histogram(portfolio_returns, bins=bin_edges)\n",
    "    datasets.append(hist/len(aapl_returns))\n",
    "\n",
    "datasets = np.array(datasets)\n",
    "\n",
    "\n",
    "# 'datasets' now contains 2000 probability distribution datasets,\n",
    "# each corresponding to a different portfolio composition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(datasets[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DistributionDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with a list of probability distributions.\n",
    "\n",
    "        Args:\n",
    "            distributions (list of list of float): List of probability distributions.\n",
    "        \"\"\"\n",
    "        self.distributions = torch.tensor(datasets, dtype=torch.float32)  # Probability distributions as torch tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of distributions in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: The number of distributions in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.distributions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a distribution by index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the distribution to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The probability distribution tensor at the given index.\n",
    "        \"\"\"\n",
    "        return self.distributions[idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "noise_size = 3\n",
    "code_size = 1\n",
    "beta = 0.15\n",
    "\n",
    "net_D = Discriminator(number_of_qubits**2, 24)\n",
    "# compiled_D = torch.compile(net_D,mode='default')\n",
    "compiled_D = net_D\n",
    "net_G = Generator()\n",
    "compiled_G = net_G\n",
    "\n",
    "batch_size = 80\n",
    "\n",
    "dataset = DistributionDataset(datasets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.rand((512, noise_size + code_size)) \n",
    "f0 = fixed_noise.clone()\n",
    "f5 = fixed_noise.clone()\n",
    "f1 = fixed_noise.clone()\n",
    "f0[:, 0] = 0.0\n",
    "f5[:, 0] = 0.5\n",
    "f1[:, 0] = 1.0\n",
    "\n",
    "lrd = 0.00004\n",
    "lrg = 0.0004\n",
    "lrt = 0.001\n",
    "gamma_d = 0.7\n",
    "gamma_g = 0.7\n",
    "gamma_t = 0.7\n",
    "\n",
    "optimizer_D = optim.Adam(compiled_D.parameters(), lr=lrd)\n",
    "scheduler_D = optim.lr_scheduler.StepLR(optimizer_D, gamma=gamma_d, step_size=30)\n",
    "optimizer_G = optim.Adam(compiled_G.parameters(), lr=lrg)\n",
    "scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, gamma=gamma_g, step_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_error = []\n",
    "g_error = []\n",
    "t_error = []\n",
    "fk_prog = []\n",
    "epochs = 450\n",
    "\n",
    "wandb.init(\n",
    "    project='InfoQGAN',\n",
    "    config = {\n",
    "        \"Generator Architecture\": str(dict(net_G.named_modules())),\n",
    "        \"Discriminator Architecture\": str(dict(net_D.named_modules())),\n",
    "        \"Batch Size\": batch_size\n",
    "    }\n",
    ")\n",
    "\n",
    "for _ in range(450):\n",
    "    for batch in dataloader:\n",
    "        # First Update Discriminator with batch of Real Data\n",
    "        d_loss = compiled_D.fit_discriminator(batch, criterion, compiled_G, optimizer_D, noise_size+code_size)\n",
    "        g_loss = compiled_G.fit_generator(compiled_D, batch_size, noise_size+code_size, criterion, optimizer_G, code_size, beta)\n",
    "\n",
    "        # print(\"Batch done\")\n",
    "        d_error.append(d_loss)\n",
    "        g_error.append(g_loss)\n",
    "        wandb.log({\"d_loss\": d_loss, \"g_loss\": g_loss})\n",
    "    \n",
    "    print(\"Epoch: \", _)\n",
    "    scheduler_D.step()\n",
    "    scheduler_G.step()\n",
    "\n",
    "    if _ % 20 == 0:\n",
    "        with torch.no_grad():\n",
    "            fake0 = compiled_G(f0)\n",
    "            fake5 = compiled_G(f5)\n",
    "            fake1 = compiled_G(f1)\n",
    "            clear_output(wait=True)\n",
    "            graph_distribution(torch.mean(fake0, dim=0).detach().numpy())\n",
    "            graph_distribution(torch.mean(fake5, dim=0).detach().numpy())\n",
    "            graph_distribution(torch.mean(fake1, dim=0).detach().numpy())\n",
    "            \n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 10 samples of data with codes between 0 and 1 in incremenets of 0.1\n",
    "noise = torch.rand(512, 4)\n",
    "noise[:,0] = 1\n",
    "samples = compiled_G(noise)\n",
    "\n",
    "sample = torch.mean(samples, dim=0).detach().numpy()\n",
    "graph_distribution(sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(compiled_G.state_dict(), './models/dauntless-surf-96.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 10 samples of data with codes between 0 and 1 in incremenets of 0.1\n",
    "def get_mean_sample(num_sampls, code):\n",
    "    with torch.no_grad():\n",
    "        noise = torch.rand(512, 4)\n",
    "        noise[:,0] = code\n",
    "        samples = compiled_G(noise)\n",
    "\n",
    "    sample = torch.mean(samples, dim=0)\n",
    "    return sample\n",
    "\n",
    "def calculate_sample_mean(sample):\n",
    "    sum = 0\n",
    "    for i in range(len(sample)):\n",
    "        return_value = (-0.1 + (0.2) * i / (len(sample))) + 0.2/32\n",
    "        sum += return_value*sample[i]\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = np.linspace(0, 1, 16)\n",
    "number_of_samples = 512\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for code in codes:\n",
    "    sample = get_mean_sample(number_of_samples, code)\n",
    "    x.append(np.std(sample.numpy()))\n",
    "    y.append(calculate_sample_mean(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot line graph of the data\n",
    "# Sample data - replace with your actual data\n",
    "data = {\n",
    "    'Standard Deviation': x,\n",
    "    'Mean': y,\n",
    "    'Code': codes\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(df['Standard Deviation'], df['Mean'], c=df['Code'], cmap='RdBu')\n",
    "plt.plot(df['Standard Deviation'], df['Mean'], c='blue', linewidth=2, alpha=0.5)\n",
    "\n",
    "\n",
    "# Adding a color bar\n",
    "plt.colorbar(label='Code')\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Mean vs Standard Deviation')\n",
    "plt.xlabel('Standard Deviation')\n",
    "plt.ylabel('Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_plot(ax, code):\n",
    "    \"\"\"\n",
    "    Graphs the distribution.\n",
    "\n",
    "    Args:\n",
    "    - distribution (torch.Tensor): The probability distribution tensor.\n",
    "    \"\"\"\n",
    "    distribution = get_mean_sample(512, code)\n",
    "    bin_edges = np.linspace(-0.1, 0.1, 17) \n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2 \n",
    "    ax.bar(bin_centers, distribution, width=bin_edges[1] - bin_edges[0], align='center', color='blue', alpha=0.7)\n",
    "    pass\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 6))\n",
    "codes = np.linspace(0,1,16)  \n",
    "\n",
    "for ax, code in zip(axes.flatten(), codes):\n",
    "    generate_plot(ax, code)\n",
    "    ax.set_title(f'code = {code:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
