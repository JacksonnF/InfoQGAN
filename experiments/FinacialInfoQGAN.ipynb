{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import pennylane as qml\n",
    "from  pennylane import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.stats import chisquare\n",
    "import plotly\n",
    "\n",
    "import numpy as npreal\n",
    "import wandb\n",
    "\n",
    "import ndtest\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hid_size) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - input_size (int): The size of the input data.\n",
    "        - hid_size (int): The size of the hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the Discriminator.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the Discriminator.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "    def fit_discriminator(self, x_data, criterion, net_G, optimizer, input_size):\n",
    "        \"\"\"\n",
    "        Trains the Discriminator module.\n",
    "\n",
    "        Args:\n",
    "        - x_data (torch.Tensor): The real data.\n",
    "        - criterion: The loss criterion.\n",
    "        - net_G: The Generator network.\n",
    "        - optimizer: The optimizer.\n",
    "        - input_size (int): The size of the input data.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, _ = x_data.shape[0], x_data.shape[1]\n",
    "        x_rand = 2*torch.rand((batch_size, input_size))-1\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "        # Forward pass Discriminator on \"real\" data\n",
    "        labels_real = torch.ones((batch_size, 1)) * 0.9\n",
    "        outputs = self.forward(x_data)\n",
    "        loss_d_real = criterion(outputs, labels_real)\n",
    "\n",
    "        # Forward pass Discriminator with \"fake\" data from Generator\n",
    "        g = net_G(x_rand).detach() # Stop gradients from being updated in generator\n",
    "        labels_fk = torch.zeros((batch_size, 1)) + 0.1\n",
    "        outputs = self.forward(g)\n",
    "        loss_d_fake = criterion(outputs, labels_fk)\n",
    "\n",
    "        loss_d = loss_d_fake + loss_d_real\n",
    "        loss_d.backward() # Compute Gradients\n",
    "        optimizer.step() # Update Weights\n",
    "        return loss_d.item()\n",
    "\n",
    "number_of_qubits = 4\n",
    "number_of_reps = 5\n",
    "\n",
    "dev = qml.device('default.qubit', wires=number_of_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_generator(inputs, params):\n",
    "    \"\"\"\n",
    "    Defines the quantum generator circuit. Embedding and Ansatz layers were based on the paper we followed.\n",
    "\n",
    "    Args:\n",
    "    - inputs (torch.Tensor): The input data.\n",
    "    - params (torch.Tensor): The parameters of the circuit.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The probabilities of the qubits.\n",
    "    \"\"\"\n",
    "    qml.AngleEmbedding(np.pi*(inputs-0.5)/2, rotation=\"Y\", wires=range(number_of_qubits))\n",
    "    num_params = 0\n",
    "    for layer in range(number_of_reps):\n",
    "        #initial Ry gates.\n",
    "        for i in range(number_of_qubits):\n",
    "            qml.RY(params[num_params], wires=i)\n",
    "            num_params += 1\n",
    "\n",
    "        # Entangling block\n",
    "        for i in range(number_of_qubits):\n",
    "            qml.CZ(wires=[i, (i+1)%number_of_qubits])\n",
    "    return qml.probs(wires=range(number_of_qubits))\n",
    "\n",
    "weights = {\"params\": number_of_qubits * number_of_reps}\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.q_gen = qml.qnn.TorchLayer(quantum_generator, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the Generator.\n",
    "\n",
    "        Args:\n",
    "        - x (torch.Tensor): The input data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the Generator.\n",
    "        \"\"\"\n",
    "        out = torch.sqrt(self.q_gen(x))\n",
    "        return out\n",
    "\n",
    "    def fit_generator(self, net_D, T, batch_size, input_size, criterion, optimizer, code_dim, beta):  \n",
    "        \"\"\"\n",
    "        Trains the Generator module.\n",
    "\n",
    "        Args:\n",
    "        - net_D: The Discriminator network.\n",
    "        - T: The T network.\n",
    "        - batch_size (int): The size of the batch.\n",
    "        - input_size (int): The size of the input data.\n",
    "        - criterion: The loss criterion.\n",
    "        - optimizer: The optimizer.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - beta (float): The beta value.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "        \"\"\"\n",
    "        x_rand = torch.rand((batch_size, input_size))\n",
    "        self.zero_grad()\n",
    "        \n",
    "        # Generate outputs With Generator and check if they fool Discriminator\n",
    "        labels_real = torch.ones((batch_size, 1)) * 0.9\n",
    "        g = self.forward(x_rand)\n",
    "        outputs = net_D(g)\n",
    "\n",
    "        loss_g = criterion(outputs, labels_real) # We want \"fake\" Generator output to look real  \n",
    "\n",
    "        # Compute the MINE loss term: -(E_pxy[T(x,y)] - log(E_pxpy[e^T(x,y)]))\n",
    "        # Note: term 1 uses joint pdf, term 2 uses marginal pdfs\n",
    "\n",
    "        x_marg = x_rand[torch.randperm(x_rand.size(0)), 0:code_dim]\n",
    "        mine = torch.mean(T(g, x_rand[:, 0:code_dim])) - torch.log(torch.mean(torch.exp(T(g, x_marg)))) \n",
    "        loss_g += -mine * beta\n",
    "\n",
    "        loss_g.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss_g.item()\n",
    "\n",
    "class T(nn.Module):\n",
    "    def __init__(self, noise_dim, code_dim, hid_size) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the T module.\n",
    "\n",
    "        Args:\n",
    "        - noise_dim (int): The dimension of the noise.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - hid_size (int): The size of the hidden layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(code_dim + noise_dim, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, gen, codes):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the T module.\n",
    "\n",
    "        Args:\n",
    "        - gen (torch.Tensor): The Generator output.\n",
    "        - codes (torch.Tensor): The code data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The output of the T module.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.layers(torch.cat((gen, codes), dim=1))\n",
    "\n",
    "    def fit_t(self, net_G, batch_size, input_size, code_dim, optimizer):\n",
    "        \"\"\"\n",
    "        Trains the T module.\n",
    "\n",
    "        Args:\n",
    "        - net_G: The Generator network.\n",
    "        - batch_size (int): The size of the batch.\n",
    "        - input_size (int): The size of the input data.\n",
    "        - code_dim (int): The dimension of the code.\n",
    "        - optimizer: The optimizer.\n",
    "\n",
    "        Returns:\n",
    "        - float: The loss value.\n",
    "        \"\"\"\n",
    "        x_rand = torch.rand((batch_size, input_size))\n",
    "        self.zero_grad()\n",
    "\n",
    "        # Compute loss following similar logic to MINE term in generator loss\n",
    "        with torch.no_grad():\n",
    "            g = net_G(x_rand)\n",
    "        \n",
    "        T_out = self.forward(g, x_rand[:, 0:code_dim])\n",
    "        x_marg = x_rand[torch.randperm(x_rand.size(0)), 0:code_dim]\n",
    "\n",
    "        t_loss = -(torch.mean(T_out) - torch.log(torch.mean(torch.exp(self.forward(g, x_marg)))))\n",
    "        t_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        return t_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
